{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Project: Wrangling a Dataset of WeRateDogs Twitter Archive\n",
    "\n",
    "## Table of Contents\n",
    "<ul>\n",
    "<li><a href=\"#intro\">Gathering Data</a></li>\n",
    "<li><a href=\"#wrangling\">Assessing Data</a></li>\n",
    "<li><a href=\"#eda\">Cleaning Data</a></li>\n",
    "<li><a href=\"#conclusions\">Storing Data</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "## Gathering Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The Data are gathereed from three different sources. \n",
    "- downloaded file named 'twitter_archive_enhanced.csv'.\n",
    "- downloaded programmatically from Udacity server \n",
    "- query using Twitter API (Tweepy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are all the libraries imported for the wrangling process:\n",
    "1. import pandas as pd\n",
    "2. import numpy as np\n",
    "3. import requests\n",
    "4. import tweepy\n",
    "5. from tweepy import OAuthHandler\n",
    "6. import json\n",
    "7. from timeit import default_timer as timer\n",
    "8. import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first source is a manually downloaded file named 'twitter_archive_enhanced.csv'. This file is simply read into dataframe using read_csv.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file is downloaded from Udacity's servers using the Requests library and the following URL: https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv\n",
    "The downloaded file exist as tsv file and loaded into the dataframe using read_csv as well but the delimiter is set to '\\t' which is tab.\n",
    "\n",
    "This file give informations on what breed of dog (or other object, animal, etc.) is present in each tweet according to a neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each tweet Json data is gathered using Twitter's API, Tweepy. The tweet IDs used are taken from the first file source. However to have access for this API, one must registered a twitter account, and then apply for a developer account. After it is approved, an app is created and only after that the Consumer API keys, Access Token and Access Token Secret can be generated. \n",
    "\n",
    "But in this case, I use the code provided in the twitter_api.py and changed all the tokens and keys required with the one that are generated. The data are stored in 'tweet_json.txt'. \n",
    "\n",
    "There are alot of informations contain for each tweet in this file. Fot this project three elements are taken into consideration. They are tweet id, retweet counts and favourites count. Json library is used to make it able to read and extracting the data. A new list that contain 3 keys for all three elements are created and everytime the data is read they are store in this list. Finally the list is converted into a dataframe and that concludes the gathering process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='wrangling'></a>\n",
    "## Assessing Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this part, all the three tables are access visually and also programmatically for quality and tidiness issues. For visual assessing, Numbers is used to go through all the data. While some pandas functions like info,uninque,shape,duplicated,head and tail are used. The issues are summarize as below:\n",
    "\n",
    "##### Quality issues:\n",
    "\n",
    "`df_clean` table\n",
    "1. Some are retweets\n",
    "2. Timestamp's datatype is object not a datetime\n",
    "3. Dogs name None,a,the,an,such,not\n",
    "4. Error rating_denominator: denominator is not 10\n",
    "5. Numerator too high\n",
    "6. Unnecessary columns for analysis : in_reply_to_status_id, in_reply_to_user_id, retweeted_status_id, retweeted_status_user_id, retweeted_status_timestamp \n",
    "7. tweets with timestamp after 1st of August 2017\n",
    "8. Some columns not necessary for this analysis\n",
    "\n",
    "`df_image_clean` table\n",
    "1. Some data do not represent dogs.(all three False predictions) eg Index : 6 is turtle;2052 is car\n",
    "2. Missing rows\n",
    "\n",
    "`df_count_clean` table\n",
    "1. missing data\n",
    "\n",
    "##### Tidiness issues:\n",
    "\n",
    "1. 1 variable in 4 columns (dog stage) in `df` table.\n",
    "2. retweet_counts and favorite_counts in `df_counts` should be in `df` table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='eda'></a>\n",
    "## Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For the pre cleaning process,a copy of each table are created and these tables are used to be cleaned instead of the original data. In this cleaning process each issue is defined in words on how to clean them, then being fixed through coding and finally tested whether the issue is resolved or not. This process is repeated for all the issues and the details of this process are describe in details in code(wrangle_act.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conclusions'></a>\n",
    "## Storing Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The main dataframe which is df_clean is merge with df_image_clean on 'tweet_id' as named as master. This master file is then converted into csv and stored as 'twitter_archive_master.csv'."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
